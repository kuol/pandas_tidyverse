-sum(percentages*posLog(percentages, base = base))
}
entropyBins(x,y)
library(sparklyr)
install.packages("sparklyr")
library(sparklyr)
?copy_to
sc <- spark_connect(master = "http://DEN-DODT-HWS-10:8998", method = "livy")
livy_install()
livy_service_start()
sc <- spark_connect(master = "http://DEN-DODT-HWS-10:8998", method = "livy")
livy_service_start()
sc <- spark_connect(master = "http://DEN-DODT-HWS-10:8998", method = "livy")
sc <- spark_connect(master = "http://DEN-DODT-HWS-10:8998", method = "livy")
livy_service_start()
library(sparklyr)
livy_service_start()
sc <- spark_connect(master = "DEN-DODT-HWS-10:8998", method = "livy")
sc <- spark_connect(master = "DEN-DODT-HWS-10:8998", method = "livy")
pkgs <- c("methods","statmod","stats","graphics","RCurl","jsonlite","tools","utils")
for (pkg in pkgs) {
if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
}
R.Version()
rm(list = ls())
install.packages("h2o")
library(h2o)
h2o.init(nthreads = -1)
csv_url <- "https://h2o-public-test-data.s3.amazonaws.com/smalldata/eeg/eeg_eyestate_splits.csv"
data <- h2o.importFile(csv_url)
dim(data)
head(data)
sapply(data, class)
is.data.frame(data)
is.matrix(data)
type(data)
str(data)
cols <- names(data)
cols
sapply(data[cols], class)
cols <- c('AF3', 'eyeDetection', 'split')
sapply(data[cols], class)
head(data[cols])
y <- 'eyeDetection'
data[y] <- as.factor(data[y])
nlevels(data[y])
h2o.nlevels(data[y])
h2o.levels(data[y])
h2o.table(data[y])
train <- data[data['split']=="train",]
valid <- data[data['split']=="valid",]
test <- data[data['split']=="test",]
model <- h2o.gbm(x = x, y = y,
training_frame = train,
validation_frame = valid,
distribution = "bernoulli",
ntrees = 100,
max_depth = 4,
learn_rate = 0.1)
x <- setdiff(names(train), c("eyeDetection", "split"))  #Remove the 13th and 14th columns
model <- h2o.gbm(x = x, y = y,
training_frame = train,
validation_frame = valid,
distribution = "bernoulli",
ntrees = 100,
max_depth = 4,
learn_rate = 0.1)
print(model)
perf <- h2o.performance(model = model, newdata = test)
class(perf)
class(data)
h2o.r2(perf)
h2o.auc(perf)
h2o.mse(perf)
?h2o.auc
print(h2o.auc(model, valid = TRUE))
?h2o.grid
gs <- h2o.grid(algorithm = "gbm",
grid_id = "eeg_demo_gbm_grid",
hyper_params = hyper_params,
x = x, y = y,
training_frame = train,
validation_frame = valid)
ntrees_opt <- c(5,50,100)
max_depth_opt <- c(2,3,5)
learn_rate_opt <- c(0.1,0.2)
hyper_params = list('ntrees' = ntrees_opt,
'max_depth' = max_depth_opt,
'learn_rate' = learn_rate_opt)
gs <- h2o.grid(algorithm = "gbm",
grid_id = "eeg_demo_gbm_grid",
hyper_params = hyper_params,
x = x, y = y,
training_frame = train,
validation_frame = valid)
print(gs)
auc_table <- h2o.getGrid(grid_id = "eeg_demo_gbm_grid", sort_by = "auc", decreasing = TRUE)
print(auc_table)
str(auc_table)
best_model <- h2o.getModel(auc_table@model_ids[[1]])
class(best_model)
h2o.auc(best_model, valid = TRUE)  #Validation AUC for best model
best_perf <- h2o.performance(model = best_model, newdata = test)
h2o.auc(best_perf)
?anova.lm
library(sparklyr)
sc <- spark_connect(master = "10.10.18.84:8998", method = "livy")
0.055317^2
sqrt(0.055317)
df_train = read.csv("~/Documents/Work/201702_Spark/Data/Boston_train_num.csv")
df_test = read.csv("~/Documents/Work/201702_Spark/Data/Boston_test_num.csv")
lm_model <- lm(y ~ rm + crim, data = df_train)
yhat <- predict(lm_model, newdata = df_test)
df_test$y-yhat
c(1,2,3)^2
rmse <- function(y, yhat) {
sqrt(mean((y-yhat)^2))
}
rmse(df_test$y , yhat)
0.0553176^2
install.packages("wordcloud")
library(wordcloud)
?wordcloud
install.packages(c('SnowballC', 'tm'))
library(tm)
library(SnowballC)
?VectorSource
docs <- c("This is a text.", "This another one.")
vs <- VectorSource(docs)
inspect(vs)
vs
inspect(Corpus(vs))
paste0("asf/sdfa/", "kliu")
?read.csv
dir_path <- "/Users/kliu/Documents/Work/201705_KeyNote/data/OpinRankDataset/hotels/las-vegas/"
file_name <- "usa_nevada_las-vegas_caesars_palace_classic_hotel"
file_path <- paste0(dir_path, file_name)
caesars <- read.csv(file_path, header = FALSE, sep = "\t")
head(caesars,1)
head(caesars,3)
ncol(caesars)
caesars <- read.csv(file_path, header = FALSE, sep = "\t", colClasses=c(NA, NA, NA, "NULL"))
head(caesars,3)
colnames(caesars) <- c("date", "title", "body")
head(caesars,3)
caesars <- read.csv(file_path, header = FALSE, sep = "\t",
col.names = c("date", "title", "body"),
colClasses=c(NA, NA, NA, "NULL"))
caesars <- read.csv(file_path, header = FALSE, sep = "\t",
col.names = c("date", "title", "body", ""),
colClasses=c(NA, NA, NA, "NULL"))
head(caesars,3)
text <- Corpus(VectorSource(caesars$body))
text <- tm_map(text, content_transformer(tolower))
inspect(text)
text <- tm_map(text, content_transformer(tolower))
text <- tm_map(text, removePunctuation)
?PlainTextDocument
text <- tm_map(text, PlainTextDocument)
text <- tm_map(text, removeWords, stopwords('english'))
caesars[11,]
?iconv
caesars <- read.csv(file_path, header = FALSE, sep = "\t",encoding = "UTF-8"
col.names = c("date", "title", "body", ""),
colClasses=c(NA, NA, NA, "NULL"))
caesars <- read.csv(file_path, header = FALSE, sep = "\t",encoding = "UTF-8",
col.names = c("date", "title", "body", ""),
colClasses=c(NA, NA, NA, "NULL"))
text <- Corpus(VectorSource(caesars$body))
text <- tm_map(text, removePunctuation)
text <- tm_map(text, PlainTextDocument)
text <- tm_map(text, removeWords, stopwords('english'))
text[11]
caesars[11]
caesars[11,]
caesars <- read.csv(file_path, header = FALSE, sep = "\t",encoding = "latin1",
col.names = c("date", "title", "body", ""),
colClasses=c(NA, NA, NA, "NULL"))
text <- Corpus(VectorSource(caesars$body))
text <- tm_map(text, removePunctuation)
text <- tm_map(text, PlainTextDocument)
text <- tm_map(text, removeWords, stopwords('english'))
text <- tm_map(text, content_transformer(tolower))
pal <- brewer.pal(9,"Reds")
pal <- pal[-(1:3)]
par(bg="transparent")
wordcloud(text, scale=c(5,0.5), max.words=100,
random.order=FALSE, rot.per=0.35,
use.r.layout=FALSE, colors=pal)
df<- read.csv(file_path, header = FALSE, sep = "\t",encoding = "latin1")
colnames(df)
df<- read.csv(file_path, sep = "\t",encoding = "latin1")
colnames(df)
?read.csv
df<- read.csv(file_path, header = TRUE, sep = "\t",encoding = "latin1")
colnames(df)
dir_path <- "/Users/kliu/Documents/Work/201705_KeyNote/data/"
file_name <- "community_data.csv"
file_path <- paste0(dir_path, file_name)
df<- read.csv(file_path, header = TRUE, sep = "\t")
df<- read.csv(file_path, header = TRUE, sep = "\t",encoding = "latin1")
df<- read.csv(file_path, header = TRUE, sep = "\t",fileEncoding = "latin1")
df<- read.csv(file_path, header = TRUE, sep = "\t",fileEncoding = "UTF-8")
df <- read.csv(file_path, header = TRUE, sep = "\t")
rm(list = ls())
dir_path <- "/Users/kliu/Documents/Work/201705_KeyNote/data/"
file_name <- "community_data.csv"
file_path <- paste0(dir_path, file_name)
df <- read.csv(file_path, header = TRUE, sep = "\t")
df <- read.csv(file_path, header = TRUE, sep = "\t", fileEncoding = "UCS-2LE")
colnames(df)
head(df,3)
text <- read.csv("/Users/kliu/Documents/Work/201705_KeyNote/topic8.csv")
text <- read.csv("/Users/kliu/Documents/Work/201705_KeyNote/top8.csv")
pal <- brewer.pal(9,"Reds")
pal <- pal[-(1:3)]
par(bg="transparent")
library(wordcloud)
wordcloud(text$word,
text$weight,
scale=c(12,1),
min.freq=3,
max.words=200,
random.order=F,
random.color=F,
rot.per=.2,
colors=pal)
pal <- brewer.pal(9,"Reds")
pal <- pal[-(1:3)]
par(bg="transparent")
wordcloud(text$word,
text$weight,
scale=c(12,1),
min.freq=3,
max.words=200,
random.order=F,
random.color=F,
rot.per=.2,
colors=pal)
wordcloud(text$word,
text$weight,
scale=c(12,1),
min.freq=1,
max.words=200,
random.order=F,
random.color=F,
rot.per=.2,
colors=pal)
wordcloud(text$word, text$weight, max.words = 10000)
wordcloud(text$word, text$weight, max.words = 10000)
rm(list = ls())
library(ggplot2)
ggplot(iris, aes(x = Sepal.Lenth, y = Sepal.Width), col = "red") + geom_point()
ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width), col = "red") + geom_point()
ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(col = "red")
ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, col = Species)) + geom_point()
ggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +
geom_point(shape = 16, size = 6, alpha = 0.6)
ggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +
geom_point(shape = 21, size = 6, alpha = 0.6)
?diff
diff(1:10, 2)
diff(1:10, 2, 2)
?range
range(iris$Sepal.Length)
?Beaver
??beaver1
library(MASS)
beaver1
str(beaver1)
library(ggplot2)
?economics
sqrt(0.15*0.85)
sqrt(0.15*0.85 / 3000) *2.57
1242/9886 - 974/10072
0.00445*1.96
d - 1242/9886 - 974/10072
d <- 1242/9886 - 974/10072
m = 0.00445*1.96
d-m
d+m
df <- data.frame(a = rnorm(10, 0, 5), b = runif(10,-5,5))
df
lapply(df, class)
smaple
?sample
sample(1:10)
sample(5)
sample(100)
library(MASS)
boston
library(MASS)
dim(Boston)
exists(mean)
exists("mean")
iris
a <- NULL
a <- 3
a
?uniroot
print(c(1,2))
?diff
a <- 1:9
a
diff(a)
diff(a,2)
diff(a,3)
a <- sample(100,10)
a
diff(a)
diff(a,2)
find("arima")
?arima.sim
?arima
install.packages(c("cluster", "ISLR", "Rtsne"))
set.seed(1680) # for reproducibility
library(dplyr) # for data cleaning
library(ISLR) # for college dataset
library(cluster) # for gower similarity and pam
library(Rtsne) # for t-SNE plot
library(ggplot2) # for visualization
library(ggplot2) # for visualization
College
head(College)
a <- random(10)
a <- rand(10)
a <- sample(100,10)
a
diff(a)
?abline
cumsum(diff(a))
cumsum(a)
rep(1,10)
cumsum(rep(1,10))
?arima.sim
?ts
qnorm(seq(0.01,0.99,0.01))
?colMeans
?sd
?pairs
pairs(iris)
a <- 1:100
a <- 1:10
a[-3]
?acf
data(Mishkin, package = "Ecdat")
install.packages("Ecdat")
data(Mishkin, package = "Ecdat")
Mishkin
set.seed(1234)
set.seed(1234)
x1 <- arima.sim(model = list(ar = 0.9), n = 100)
x2 <- arima.sim(model = list(order = c(0.9,0,0)), n = 100)
x1
?arima.sim
?residuals
Arima.residuals
?residuals.Arima
?residuals.arima
?predict.Arima
?args
library(forecast)
args(ses)
?fitted
fitted.forecast
forecast.fitted
ftype(fitted
)
install.packages(pryr)
install.packages('pryr')
library(pryr)
ftype(fitted)
methods(fitted)
?fitted.ets
a <- 1:10
a[-5:]
a[-5]
a[5:]
subset.ts
library(forecast)
subset.ts
?subset.ts
library(lubridate)
library(lubridate)
dates <- c("April-29-2014", "April-23-2013", "September-30-2012")
mdy(dates)
x <- na
x <- # NA
x <- NA
x
df <- data.frame(x = c(41,53,35,NA), y = c(NA, 32, NA, 26))
df
complete.cases(df)
is.na(df)
na.omit(df)
glimpse(matcars)
library(tidyr)
glimpse(matcars)
library(dplyr)
glimpse(matcars)
glimpse(mtcars)
str(mtcars)
gather(cases, "year", "n", 2:4)
?gather
stocks <- data_frame(
time = as.Date('2009-01-01') + 0:9,
X = rnorm(10, 0, 1),
Y = rnorm(10, 0, 2),
Z = rnorm(10, 0, 4)
)
stocks
gather(stocks, stock, price, -time)
ls()
list.files()
setwd("~/Documents/Work/201805_Inspire")
# Tidyr ============
df1 <- read.csv("supplies.csv")
list.files()
setwd("~/Documents/Work/201805_Inspire/code")
# Tidyr ============
df1 <- read.csv("supplies.csv")
df1 <- as_tibble(df1)
library(dplyr)
df1 <- as_tibble(df1)
df1
# Tidyr ============
library(tidyr)
df1 %>% gather(key = Product:Average.Monthly.Sales, data = January:December )
df1 %>% gather(key = c(Product, Category, Average.Monthly.Sales), data = January:December )
df1 %>% gather(key = Product, data = January:December )
df1 %>% gather(key = Product, data = January:December )
df1 %>% gather(key = Product, data = Months, January:December )
?gather
df1 %>% gather(key = Product, value = Month, January:December )
df1 %>% gather(Product, months, January:December )
gather(df1, Product, months, January:December )
gather(df1, Name, Value, January:December )
gather(df1, Name, Value, January:December )
dim(df1)
# Spread (Alteryx's crosstab) =======
df2 <- read.csv("presidents.csv")
df2 <- as_tibble(df2)
df2
df2 %>% spread(key = Pres_or_VP, value = Name)
?spread
df2 %>% spread(key = Pres_or_VP, value = Name, sep = ',')
df2 %>% spread(key = Pres_or_VP, value = Name) %>%
select(presidentNo, President, `Vice President`)
df2 %>% select(presidentNo, Pres_or_VP, Name) %>%
spread(key = Pres_or_VP, value = Name)
df2 %>% select(presidentNo, Pres_or_VP, Name) %>%
spread(key = Pres_or_VP, value = Name)
df2 %>% select(presidentNo, Pres_or_VP, Name)
df2 %>% select(presidentNo, Pres_or_VP, Name) %>% group_by(presidentNo) %>% (count = ())
df2 %>% select(presidentNo, Pres_or_VP, Name) %>% group_by(presidentNo) %>% summarise(count = ())
df2 %>% select(presidentNo, Pres_or_VP, Name) %>% group_by(presidentNo) %>% summarise(count = n())
dff <- df2 %>% group_by(presidentNo)
dff[1]
dff
weather3
library(tidyverse)
install.packages("tidyverse")
install.packages("tidyverse")
table1
library(tidyr)
# Gather (Alteryx's transpose) ======
df1 <- read.csv("supplies.csv")
df1 <- as_tibble(df1)
df2 <- df1 %>% gather(Name, Value, January:December )
df2
df2 %>% spread(key = Name, value = Value)
months
months()
month
col_names <- names(df2)
col_names
col_names <- names(df1)
col_names
df2 %>% spread(key = Name, value = Value) %>%
select(names(df1))
df2 %>% spread(key = Name, value = Value)
my_col_names <- names(df1)
my_col_names
df2 %>% spread(key = Name, value = Value) %>%
select(unlist(my_col_names)
)
df3 <- df2 %>% spread(key = Name, value = Value)
df3 %>% select(unlist(my_col_names))
library(dplyr)
df3 %>% select(unlist(my_col_names))
df2 %>% spread(key = Name, value = Value) %>% select(unlist(my_col_names))
df2 %>% spread(key = Name, value = Value) %>% select(my_col_names)
df2 %>% spread(key = Name, value = Value) %>% select(names(df1))
library(tidyr)
# Gather (Alteryx's transpose) ======
df1 <- read.csv("supplies.csv")
df1 <- as_tibble(df1)
df2 <- df1 %>% gather(Name, Value, January:December )
df2
# Spread (Alteryx's crosstab) =======
df2 %>% spread(key = Name, value = Value) %>% select(names(df1))
mtcars
sessionInfo()
artists0
artists
?join
band_members
band_instruments
band_members %>% inner_join(band_instruments)
band_members %>% left_join(band_instruments)
band_members %>% right_join(band_instruments)
band_members %>% full_join(band_instruments)
